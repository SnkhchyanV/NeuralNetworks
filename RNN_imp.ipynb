{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwg7l/vMiP5OW7eNF9ToZS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SnkhchyanV/NeuralNetworks/blob/main/RNN_imp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "I3Fd-s912GYV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sGBnQusU19pJ"
      },
      "outputs": [],
      "source": [
        "class RNN():\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.W_xh = np.random.normal(size=(input_size, hidden_size))\n",
        "    self.W_hh = np.random.normal(size=(hidden_size, hidden_size))\n",
        "    self.W_hy = np.random.normal(size=(hidden_size, output_size))\n",
        "    self.b_h = np.zeros((1, hidden_size))\n",
        "    self.b_y = np.zeros((1, output_size))\n",
        "\n",
        "  def __forward(self, x_i, h_prev):\n",
        "    temp =  x_i @ self.W_xh  +  h_prev @ self.W_hh + self.b_h\n",
        "    h_i = np.tanh(temp)\n",
        "    y_i = h_i @ self.W_hy + self.b_y\n",
        "    return h_i, y_i\n",
        "\n",
        "  def __backward(self, X, y, hidden_states):\n",
        "    dW_xh = np.zeros_like(self.W_xh)\n",
        "    dW_hh = np.zeros_like(self.W_hh)\n",
        "    dW_hy = np.zeros_like(self.W_hy)\n",
        "    db_h = np.zeros_like(self.b_h)\n",
        "    db_y = np.zeros_like(self.b_y)\n",
        "    dh_next = np.zeros((1, self.hidden_size))\n",
        "\n",
        "    for i in reversed(range(X.shape[0])):\n",
        "      dy = y[i]\n",
        "      dy -= y[i, np.argmax(y[i])]\n",
        "      dW_hy += np.outer(hidden_states[i + 1], dy)\n",
        "      db_y += dy\n",
        "      dh = np.dot(dy, self.W_hy.T) + dh_next\n",
        "      dh_raw = (1 - hidden_states[i + 1] ** 2) * dh\n",
        "      db_h += dh_raw\n",
        "      dW_xh += np.outer(X[i], dh_raw)\n",
        "      dW_hh += np.outer(hidden_states[i], dh_raw)\n",
        "      dh_next = np.dot(dh_raw, self.W_hh.T)\n",
        "\n",
        "    gradients = [dW_xh, dW_hh, dW_hy, db_h, db_y]\n",
        "    for g in gradients:\n",
        "      np.clip(g, -1, 1, out=g)\n",
        "\n",
        "    return dW_xh, dW_hh, dW_hy, db_h, db_y\n",
        "\n",
        "\n",
        "  def train(self, X, y, epochs=20, lr=0.001):\n",
        "    for e in range(epochs):\n",
        "      hidden_states = np.zeros((X.shape[0], X.shape[1] + 1, self.hidden_size))\n",
        "      output_array = np.zeros((X.shape[0], X.shape[1], self.output_size))\n",
        "\n",
        "      for sample in range(X.shape[0]):\n",
        "        for i in range(X.shape[1]):\n",
        "          hidden_states[sample, i + 1], output_array[sample, i] = self.__forward(X[sample, i], hidden_states[sample, i])\n",
        "\n",
        "        dW_xh, dW_hh, dW_hy, db_h, db_y = self.__backward(X[sample], output_array[sample], hidden_states[sample])\n",
        "        self.W_xh -= lr * dW_xh\n",
        "        self.W_hh -= lr * dW_hh\n",
        "        self.W_hy -= lr * dW_hy\n",
        "        self.b_h -= lr * db_h\n",
        "        self.b_y -= lr * db_y\n",
        "\n",
        "    return output_array, hidden_states\n",
        "\n",
        "  def generate(self, seed, length):\n",
        "    hidden_state = np.zeros((1, self.hidden_size))\n",
        "    output_sequence = []\n",
        "\n",
        "    for _ in range(length):\n",
        "      hidden_state, output = self.__forward(seed, hidden_state)\n",
        "      output_sequence.append(output)\n",
        "      seed = output\n",
        "\n",
        "    return output_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "string = \"This book is a comprehensive introduction to text forming resources in English, along with practical procedures for analysing English texts and relating them to their contexts of use. It has been designed to complement functional grammars of English, building on the generation of discourse analysis inspired by Halliday and Hasan's Cohesion in English. The analyses presented were developed within three main theoretical and applied contexts: (i) educational linguistics (especially genre-based literacy programmes) (ii) critical linguistics (as manifested in the development of social semiotics) and (iii) computational linguistics (in dialogue with the various text generation projects based on systemic approaches to grammar and discourse). English Text's major contribution is to outline one way in which a rich semantically oriented functional grammar can be systematically related to a theory of discourse semantics, including deconstruction of contextual issues (i.e. register, genre and ideology). The chapters have been organized with the needs of undergraduate students in theoretical linguistics and postgraduate students in applied linguistics in mind.\"\n",
        "sequences_list = re.split(\"[.,]\", string)\n",
        "\n",
        "\n",
        "m = 53  # +26 lower case characters, +26 upper case characters, and +1 for space\n",
        "character = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \")\n",
        "data = list()\n",
        "labels = list()\n",
        "for sequence in sequences_list:\n",
        "  seq = np.array((list(sequence)))\n",
        "  n = seq.size\n",
        "\n",
        "  sequence_matrix = np.zeros((n,m))\n",
        "  for i in range(n):\n",
        "    for j in range(len(character)):\n",
        "      if(seq[i]==character[j]):\n",
        "        sequence_matrix[i][j] = 1\n",
        "        break\n",
        "\n",
        "  data.append(np.array(sequence_matrix))\n",
        "\n",
        "  target_matrix = np.zeros((n,m))\n",
        "  for i in range(n-1):\n",
        "      target_matrix[i] = sequence_matrix[i+1]\n",
        "\n",
        "  labels.append(np.array(target_matrix))\n",
        "\n",
        "\n",
        "data = np.array(data, dtype='object')\n",
        "labels = np.array(labels, dtype='object')"
      ],
      "metadata": {
        "id": "euuKfIbH2FLc"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(data)\n",
        "# print(labels)"
      ],
      "metadata": {
        "id": "URo3CSjEAJow"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10\n",
        "RNN_O = RNN(m,k,m)\n",
        "# first m is input size OR every sequence size, in our situation it is onehot encoded, value 1 of each vector of encoded matrix indicates charachter in dict,\n",
        "# output size is also m, it is the probability of every symbol\n",
        "# k is hidden layer size, it means how many characters model will remember at each iteration, itw it is number of cells in layer\n",
        "# so this model should be able to predict every next character for every 5 symbols\n",
        "\n",
        "RNN_O.train(data, labels)\n",
        "print(\"Nothing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zuZPFTDNvyOm",
        "outputId": "fe6ec1b9-78af-4555-83b9-48d6d41ff2a2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-3491c404cbe1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# so this model should be able to predict every next character for every 5 symbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mRNN_O\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nothing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-5be6d8daefbc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, epochs, lr)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;31m#print(X.shape[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m       \u001b[0moutput_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nq_aFeJ32UY8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}