{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4bWWWru7sbOoVSkwunvaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SnkhchyanV/NeuralNetworks/blob/main/RNN_imp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "I3Fd-s912GYV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "sGBnQusU19pJ"
      },
      "outputs": [],
      "source": [
        "class RNN():\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.W_xh = np.random.normal(size=(input_size, hidden_size))\n",
        "    self.W_hh = np.random.normal(size=(hidden_size, hidden_size))\n",
        "    self.W_hy = np.random.normal(size=(hidden_size, output_size))\n",
        "    self.b_h = np.zeros((1, hidden_size))\n",
        "    self.b_y = np.zeros((1, output_size))\n",
        "\n",
        "  def __forward(self, x_i, h_prev):\n",
        "    temp =  x_i @ self.W_xh  +  h_prev @ self.W_hh + self.b_h\n",
        "    h_i = np.tanh(temp)\n",
        "    y_i = h_i @ self.W_hy + self.b_y\n",
        "    return h_i, y_i\n",
        "\n",
        "  def __backward(self, X, y, hidden_states):\n",
        "    dW_xh = np.zeros_like(self.W_xh)\n",
        "    dW_hh = np.zeros_like(self.W_hh)\n",
        "    dW_hy = np.zeros_like(self.W_hy)\n",
        "    db_h = np.zeros_like(self.b_h)\n",
        "    db_y = np.zeros_like(self.b_y)\n",
        "    dh_next = np.zeros((1, self.hidden_size))\n",
        "\n",
        "    for i in reversed(range(X.shape[0])):\n",
        "      dy = y[i]\n",
        "      dy -= y[i, np.argmax(y[i])]\n",
        "      dW_hy += np.outer(hidden_states[i + 1], dy)\n",
        "      db_y += dy\n",
        "      dh = np.dot(dy, self.W_hy.T) + dh_next\n",
        "      dh_raw = (1 - hidden_states[i + 1] ** 2) * dh\n",
        "      db_h += dh_raw\n",
        "      dW_xh += np.outer(X[i], dh_raw)\n",
        "      dW_hh += np.outer(hidden_states[i], dh_raw)\n",
        "      dh_next = np.dot(dh_raw, self.W_hh.T)\n",
        "\n",
        "    gradients = [dW_xh, dW_hh, dW_hy, db_h, db_y]\n",
        "    for g in gradients:\n",
        "      np.clip(g, -1, 1, out=g)\n",
        "\n",
        "    return dW_xh, dW_hh, dW_hy, db_h, db_y\n",
        "\n",
        "\n",
        "  def train(self, X, y, epochs=20, lr=0.001):\n",
        "    for e in range(epochs):\n",
        "      hidden_states = np.zeros((X.shape[0], X.shape[1] + 1, self.hidden_size))\n",
        "      output_array = np.zeros((X.shape[0], X.shape[1], self.output_size))\n",
        "\n",
        "      for sample in range(X.shape[0]):\n",
        "        for i in range(X.shape[1]):\n",
        "          hidden_states[sample, i + 1], output_array[sample, i] = self.__forward(X[sample, i], hidden_states[sample, i])\n",
        "\n",
        "        dW_xh, dW_hh, dW_hy, db_h, db_y = self.__backward(X[sample], output_array[sample], hidden_states[sample])\n",
        "        self.W_xh -= lr * dW_xh\n",
        "        self.W_hh -= lr * dW_hh\n",
        "        self.W_hy -= lr * dW_hy\n",
        "        self.b_h -= lr * db_h\n",
        "        self.b_y -= lr * db_y\n",
        "\n",
        "    return output_array, hidden_states\n",
        "\n",
        "  def generate(self, seed, length):\n",
        "    hidden_state = np.zeros((1, self.hidden_size))\n",
        "\n",
        "    for _ in range(length):\n",
        "      hidden_state, output = self.__forward(seed, hidden_state)\n",
        "      seed = output\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def onehot_encoder(string, encoding_dict):\n",
        "\n",
        "  sequences_list = re.split(\"[.,]\", string)\n",
        "  m = len(encoding_dict)\n",
        "  character = encoding_dict\n",
        "  data = list()\n",
        "  labels = list()\n",
        "\n",
        "  k = 30\n",
        "  i = 0\n",
        "  for sequence in sequences_list:\n",
        "    if len(sequence)>k:\n",
        "      sequences_list[i] = \" \".join(sequence[i] for i in range(k))\n",
        "\n",
        "    elif len(sequence)<k:\n",
        "      spaces_to_add = k - len(sequence)\n",
        "      sequences_list[i] = sequence + ' ' * spaces_to_add\n",
        "\n",
        "    i+=1\n",
        "\n",
        "  data = np.zeros((len(sequences_list), k, m))\n",
        "  labels = np.zeros((len(sequences_list), k, m))\n",
        "  l = 0\n",
        "  for sequence in sequences_list:\n",
        "    seq = np.array((list(sequence)))\n",
        "\n",
        "    sequence_matrix = np.zeros((k,m))\n",
        "    for i in range(k):\n",
        "      for j in range(len(character)):\n",
        "        if(seq[i]==character[j]):\n",
        "          sequence_matrix[i][j] = 1\n",
        "          break\n",
        "    data[l] = sequence_matrix\n",
        "\n",
        "    target_matrix = np.zeros((k,m))\n",
        "    for i in range(k-1):\n",
        "        target_matrix[i] = sequence_matrix[i+1]\n",
        "    labels[l] = target_matrix\n",
        "    l+=1\n",
        "  return data, labels"
      ],
      "metadata": {
        "id": "euuKfIbH2FLc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(data)\n",
        "# print(labels)"
      ],
      "metadata": {
        "id": "URo3CSjEAJow"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10\n",
        "\n",
        "encoding_dict = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \")\n",
        "m = len(encoding_dict)\n",
        "RNN_O = RNN(m,k,m)\n",
        "# first m is input size OR every sequence size, in our situation it is onehot encoded, value 1 of each vector of encoded matrix indicates charachter in dict,\n",
        "# output size is also m, it is the probability of every symbol\n",
        "# k is hidden layer size, it means how many characters model will remember at each iteration, itw it is number of cells in layer\n",
        "# so this model should be able to predict every next character for every 5 symbols\n",
        "\n",
        "string = \"This book is a comprehensive introduction to text forming resources in English, along with practical procedures for analysing English texts and relating them to their contexts of use. It has been designed to complement functional grammars of English, building on the generation of discourse analysis inspired by Halliday and Hasan's Cohesion in English. The analyses presented were developed within three main theoretical and applied contexts: (i) educational linguistics (especially genre-based literacy programmes) (ii) critical linguistics (as manifested in the development of social semiotics) and (iii) computational linguistics (in dialogue with the various text generation projects based on systemic approaches to grammar and discourse). English Text's major contribution is to outline one way in which a rich semantically oriented functional grammar can be systematically related to a theory of discourse semantics, including deconstruction of contextual issues (i.e. register, genre and ideology). The chapters have been organized with the needs of undergraduate students in theoretical linguistics and postgraduate students in applied linguistics in mind.\"\n",
        "\n",
        "\n",
        "data, labels = onehot_encoder(string, encoding_dict)\n",
        "\n",
        "RNN_O.train(data, labels)\n",
        "print(\"Nothing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuZPFTDNvyOm",
        "outputId": "2283c2c0-60f4-487a-8216-27578474b45a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nothing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Can you give me something\"\n",
        "data, _ = onehot_encoder(prompt, encoding_dict)\n",
        "\n",
        "sample = RNN_O.generate(data,50)\n",
        "\n",
        "def sample_decoder(sample, encoding_dict):\n",
        "  output = \"\"\n",
        "  for seq in sample[0]:\n",
        "    for j in range(len(encoding_dict)):\n",
        "      if np.argmax(seq)==j:\n",
        "        output = \"\".join((output,encoding_dict[j]))\n",
        "  return output\n",
        "\n",
        "print(sample_decoder(sample, encoding_dict))\n"
      ],
      "metadata": {
        "id": "nq_aFeJ32UY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9018b04-4cb5-4f56-f1b1-3e686adda637"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nCLnLCHnLLCHnLHnCCLHCHLLLnnnnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81bdmjDk1Wwx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}